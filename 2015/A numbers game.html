<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<title>A numbers game</title>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<style>
			body { 
				font-family: sans-serif; 
				font-size: 16px; 
				line-height: 1.7; 
				text-rendering: optimizeLegibility; 
				max-width: 600px;
				margin-top: 40px;
				margin-right: auto;
				margin-left: auto;
				margin-bottom: 40px;
				-webkit-font-smoothing:antialiased;
			}
			h1 { 
				font-size: 24px; 
				line-height: 28px; 
				font-weight: 700; 
				margin-bottom: 5px;
				text-decoration: none; 
				border: none; 
				color: #333;
			}
			h1 a {
				border-bottom: none;
			}
			h2 {
				font-size: 18px;
				margin-top: 30px;
				margin-bottom: 0px;
			}
			p {
				color: #333;
			}
			a { 
				color: inherit; 
				text-decoration: none; 
				border-bottom: 
				1px dotted; 
			}
			a:hover {
				border-bottom: 1px solid; 
			}
			img { 
				page-break-inside: avoid; 
				max-width: 600px;
			}
			.source {
				margin-bottom: 20px;
				font-size: 12px;
				color: #777;
			}
			p.source {
				margin-top: 0px;
			}
			.source a {
				border-bottom: none;
			}
			.source a:hover {
				border-bottom: 1px solid #B4B4B4;
				color: #777;
			}
			.bottom-links {
				border-top: 1px solid #C2C2C2;
				margin-top: 40px;
				padding-top: 10px;
				color: #777;
			}
		</style>
<!-- Article JSON data
<script>
var article = {"id":"BJi9nUvd4jCSd9rf0lPNeaPIDvhjO/Py+q6EVKn1X88=_14e6eb280df:10328fc9:cd74fcc6","fingerprint":"9ddb561c","originId":"http://dx.doi.org/10.1038/523127b","content":{"content":"<p>\n<b>A numbers game</b>\n</p>\n<p>Nature 523, 7559 (2015). <a href=\"http://dx.doi.org/10.1038/523127b\">doi:10.1038/523127b</a>\n</p>\n<p>Institutions must be plain about research metrics if academics are to engage with them.</p>\n<img height=\"1\" alt=\"\" width=\"1\" src=\"http://feeds.feedburner.com/~r/nature/rss/current/~4/uCmSeF1wcn4\">","direction":"ltr"},"title":"A numbers game","published":1436338800000,"crawled":1436376269023,"alternate":[{"href":"http://feeds.nature.com/~r/nature/rss/current/~3/uCmSeF1wcn4/523127b","type":"text/html"}],"canonical":[{"href":"http://dx.doi.org/10.1038/523127b","type":"text/html"}],"origin":{"streamId":"feed/http://www.nature.com/nature/current_issue/rss","title":"Nature - Issue - nature.com science feeds","htmlUrl":"http://www.nature.com/nature/current_issue/"},"summary":{"content":"Institutions must be plain about research metrics if academics are to engage with them.","direction":"ltr"}};
</script>
-->
	</head>
	<body>
		<div class="article-header">
			<h1><a target="_blank" href="http://dx.doi.org/10.1038/523127b">A numbers game</a></h1>
			<p class="source"><a target="_blank" href="http://www.nature.com/nature/current_issue/">Nature - Issue - nature.com science feeds</a>
            </p>
		</div>
		<div id="article-body" class="article-body instapaper_body entry-content">
		<div><div class="content no-heading cleared main-content">

															<p>Scientists like to grumble about the peer-review system for judging research quality, but there is one sure way to make most of them defend it: suggest that peer review should be replaced with numerical measures of academic output.</p>																				<p>A major UK report on the use of such research metrics this week reinforces this defence of the status quo (see <a href="http://go.nature.com/smbaix">go.nature.com/smbaix</a>). Metrics, it concludes, are not yet ready to replace peer review as the preferred way to judge research papers, proposals and individuals.</p>																				<p>Even if such metrics do not replace peer review in all situations, will they ever be ready to make a serious and trusted contribution to the assessment of science and scientists? As James Wilsdon, lead author of the UK report, writes in a World View on <a href="http://www.nature.com/uidfinder/10.1038/523129a">page 129</a>, the one certainty in this debate is that the lure of metrics will only increase. Scientists should not stick their heads in the sand and pretend that the issue will go away. Rather, they should engage with metrics and work to improve the evidence base for them.</p>																				<p>British universities now track the output of their academic staff using systems to gather details about their funding and types of output — patents, papers, citations and research grants — and to analyse institutional strengths for comparison with rival universities.</p>																				<p>A sophisticated infrastructure has sprung up to support this activity. But it is patchy and inconsistent, with university managers often hopping between various approaches. Some, for example, have built their own internal research-information systems, and others rely on online databases of researcher outputs collected by funding agencies. There are non-profit systems that use public information, and commercially owned databases of bibliometric citations. A host of commercial benchmarking services can analyse the information. These analytical services are becoming increasingly sophisticated. They feature many different ways to group citation metrics, to cover collections of papers by individual, department, institution or journal, and to benchmark them against similar groups.</p>																														
			
	<div class="pullquote pullquote-left">
		<div class="pullquote-sleeve">
						<blockquote>
				<p>“It is essential that universities are open about the metrics that they build and use.”</p>
			</blockquote>
		</div>
	</div>
																										<p>The problem is that most of these metrics tools lack transparency. At the heart of the system, databases of academic outputs and citations are not publicly accessible or auditable. And the indicators built on top of these databases can also be black boxes: the UK report notes that there are no fewer than ten major global rankings of universities, for example. Some use poorly explained scores and arbitrary weightings to underpin their league tables, and as the report says, they “assume degrees of objectivity, authority and precision that are not yet possible to achieve in practice”. To some extent, metrics are used and quoted simply because other universities use them — the supply of league tables creates its own demand.</p>																				<p>Such opacity can lead to distrust, negating the very advantage of metrics over qualitative assessment as objective, open measures of research performance. It is essential, therefore, that universities are open about the metrics that they build and use.</p>																				<p>Transparency is one of the hallmarks of ‘responsible metrics’, a term introduced by the report that covers principles such as using robust data and applying diverse indicators that account for variation by field and for multiple research types. Other principles include being humble about the limits of quantitative evaluation — which the report notes should support, rather than replace, expert assessment — and recognizing that indicators must change over time.</p>																				<p>Although it seems legitimate to use a range of metrics to analyse research performance, their use as managerial targets can leave academics feeling ‘painted by numbers’ — requiring them to change their behaviour to meet often-arbitrary goals. Institutions should therefore publicly state their principles to research managers and explain why they are using particular indicators as a management tool, as the report recommends. Perhaps the most important aspect to recognize about metrics is that they can make judgements more objective — but they can also objectify those being judged.</p>												
			</div>
			
			</div>
		</div>
		<div class="bottom-links">
			Saved from <a target="_blank" href="http://feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Fwww.nature.com%2Fnature%2Fcurrent_issue%2Frss">Nature - Issue - nature.com science feeds</a> on feedly
		</div>
	</body>
</html>
